{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "806eeeb7-0738-4824-b2c5-9e5c92a7332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv('base_calculation_file.csv')\n",
    "\n",
    "\n",
    "#generating the elasticity dataset\n",
    "brand_arr = [[]]\n",
    "brands = df['Article#'].unique()\n",
    "for brand in brands:\n",
    "    # print (brand)\n",
    "    mop = df[df['Article#']==brand]['MOP'].iloc[0].astype(int)\n",
    "    nlc = df[df['Article#']==brand]['NLC'].iloc[0].astype(int)\n",
    "    m = df[df['Article#']==brand]['m'].iloc[0]\n",
    "    c = df[df['Article#']==brand]['c'].iloc[0]\n",
    "    # brand_arr.append(brand)\n",
    "    for price in range (nlc,mop,400):\n",
    "        quantity = price*m + c\n",
    "        brand_arr.append([brand,price,quantity])       \n",
    "    df_pde = pd.DataFrame(brand_arr)\n",
    "df_pde.rename(columns = {0:'Article#',1:'Price',2:'Units'},inplace=True)\n",
    "df_pde.dropna(inplace=True)\n",
    "\n",
    "\n",
    "df = df[['Article#','MOP','NLC']]\n",
    "\n",
    "df_elasticity = df.merge(df_pde, on = 'Article#')\n",
    "df_elasticity['Units'] = df_elasticity['Units'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#create calculated columns\n",
    "df_elasticity['Discount'] = df_elasticity['MOP']-df_elasticity['Price']\n",
    "df_elasticity['Discount_%'] = round(100*((df_elasticity['MOP']-df_elasticity['Price'])/df_elasticity['MOP']),2)\n",
    "df_elasticity['Discount_Per_Unit'] = round(df_elasticity['Discount']/df_elasticity['Units'],2)\n",
    "df_elasticity['GP_per_unit'] = df_elasticity['Price'] - df_elasticity['NLC']\n",
    "df_elasticity['GP'] = df_elasticity['GP_per_unit'] * df_elasticity['Units']\n",
    "df_elasticity['GMV'] = df_elasticity['Price'] * df_elasticity['Units']\n",
    "df_elasticity['GP_%'] = round(100*(df_elasticity['GP']/df_elasticity['GMV']),2)\n",
    "\n",
    "from itertools import product\n",
    "# Separate into DataFrames per brand\n",
    "brands = df_elasticity['Article#'].unique()\n",
    "dfs = {brand: df_elasticity[df_elasticity['Article#'] == brand].reset_index(drop=True) for brand in brands}\n",
    "\n",
    "# Get row indices for each brand\n",
    "brand_rows = [list(range(len(dfs[brand]))) for brand in brands]\n",
    "\n",
    "# Cartesian product of row indices\n",
    "all_combinations = list(product(*brand_rows))\n",
    "\n",
    "# For each combination of row indices, build a row by horizontally joining brand rows\n",
    "combined_rows = []\n",
    "for row_idxs in all_combinations:\n",
    "    row_parts = [dfs[brand].iloc[[idx]].reset_index(drop=True) for brand, idx in zip(brands, row_idxs)]\n",
    "    combined_row = pd.concat(row_parts, axis=1)\n",
    "    combined_rows.append(combined_row)\n",
    "\n",
    "# Combine all rows into the final dataframe\n",
    "final_df = pd.concat(combined_rows, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Optional: Clean up column names\n",
    "new_cols = []\n",
    "for brand in brands:\n",
    "    new_cols.extend([f\"{col}_{brand}\" for col in dfs[brand].columns])\n",
    "final_df.columns = new_cols\n",
    "\n",
    "# Sum GMV and GP columns\n",
    "def sum_columns_by_prefix(df, prefix, new_col_name):\n",
    "    cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    df[new_col_name] = df[cols].sum(axis=1)\n",
    "    return df\n",
    "\n",
    "# Average GP_per columns\n",
    "def mean_columns_by_prefix(df, prefix, new_col_name):\n",
    "    cols = [col for col in df.columns if col.startswith(prefix)]\n",
    "    df[new_col_name] = df[cols].apply(pd.to_numeric, errors='coerce').mean(axis=1)\n",
    "    return df\n",
    "\n",
    "# Apply transformations\n",
    "final_df = sum_columns_by_prefix(final_df, \"GMV_\", \"Total_GMV\")\n",
    "final_df = sum_columns_by_prefix(final_df, \"GP_\", \"Total_GP\")\n",
    "final_df = mean_columns_by_prefix(final_df, \"GP_%\", \"Avg_GP_per\")\n",
    "final_df.to_csv('universe_of_combination.csv',index=False)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d39a67a-681e-46f6-bb08-0223d6b2a5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Step 2: Split into two smaller dataframes\n",
    "mid_point = len(final_df) // 2\n",
    "final_df_1 = final_df.iloc[:mid_point].reset_index(drop=True)\n",
    "final_df_2 = final_df.iloc[mid_point:].reset_index(drop=True)\n",
    "\n",
    "# Step 3: Save the two smaller dataframes\n",
    "final_df_1.to_csv('universe_of_combination_part1.csv', index=False)\n",
    "final_df_2.to_csv('universe_of_combination_part2.csv', index=False)\n",
    "\n",
    "# Step 4: Delete the original big file\n",
    "os.remove('universe_of_combination.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc9af7a-ec10-48b3-bb71-315861332ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
